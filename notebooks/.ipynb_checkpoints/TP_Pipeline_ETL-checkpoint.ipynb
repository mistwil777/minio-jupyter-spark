{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877fcaa8",
   "metadata": {},
   "source": [
    "## üì¶ √âtape 0 : Installation et upload du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de boto3\n",
    "!pip install boto3 -q\n",
    "print(\"‚úÖ boto3 install√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "# Configuration du client S3 pour MinIO\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://minio:9000',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin',\n",
    "    config=Config(signature_version='s3v4'),\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Client boto3 configur√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6371ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le bucket datalake\n",
    "bucket = 'datalake'\n",
    "\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket)\n",
    "    print(f\"‚úÖ Bucket '{bucket}' existe\")\n",
    "except:\n",
    "    s3.create_bucket(Bucket=bucket)\n",
    "    print(f\"‚úÖ Bucket '{bucket}' cr√©√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b42baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er et uploader le fichier CSV\n",
    "csv_content = \"\"\"id,produit,prix,quantite\n",
    "1,Stylo,1.20,10\n",
    "2,Cahier,2.50,5\n",
    "3,Gomme,0.80,20\n",
    "4,Stylo,1.20,15\"\"\"\n",
    "\n",
    "s3.put_object(\n",
    "    Bucket='datalake',\n",
    "    Key='bronze/ventes/ventes.csv',\n",
    "    Body=csv_content,\n",
    "    ContentType='text/csv'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fichier CSV upload√© dans datalake/bronze/ventes/ventes.csv\")\n",
    "\n",
    "# V√©rification\n",
    "obj = s3.get_object(Bucket='datalake', Key='bronze/ventes/ventes.csv')\n",
    "content = obj['Body'].read().decode('utf-8')\n",
    "print(\"\\nüìÑ Contenu du fichier upload√© :\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e7518",
   "metadata": {},
   "source": [
    "## ‚ú® √âtape 1 : Connexion PySpark ‚Üî MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a628e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"TP Bronze Silver Gold\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ SparkSession cr√©√©e - Version : {spark.version}\")\n",
    "print(f\"üìä Spark UI : http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276691b",
   "metadata": {},
   "source": [
    "## ü•â √âtape 2 : Layer Bronze - Ingestion brute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du CSV depuis MinIO (sans inf√©rence de sch√©ma pour √©viter les erreurs)\n",
    "df_bronze = spark.read.csv(\n",
    "    \"s3a://datalake/bronze/ventes/ventes.csv\",\n",
    "    header=True,\n",
    "    inferSchema=False  # On lit tout en String d'abord\n",
    ")\n",
    "\n",
    "print(\"ü•â BRONZE - Donn√©es brutes :\")\n",
    "df_bronze.show()\n",
    "df_bronze.printSchema()\n",
    "print(f\"\\nüìä {df_bronze.count()} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cf6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde Bronze en Parquet\n",
    "df_bronze.write.mode(\"overwrite\").parquet(\"s3a://datalake/bronze/ventes/parquet/\")\n",
    "print(\"‚úÖ Bronze sauvegard√© en Parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357fcd2",
   "metadata": {},
   "source": [
    "## ü•à √âtape 3 : Layer Silver - Nettoyage + typage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "# Typage et nettoyage\n",
    "df_silver = df_bronze \\\n",
    "    .withColumn(\"id\", col(\"id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"prix\", col(\"prix\").cast(DoubleType())) \\\n",
    "    .withColumn(\"quantite\", col(\"quantite\").cast(IntegerType())) \\\n",
    "    .dropna() \\\n",
    "    .withColumn(\"montant_total\", col(\"prix\") * col(\"quantite\"))\n",
    "\n",
    "print(\"ü•à SILVER - Donn√©es nettoy√©es :\")\n",
    "df_silver.show()\n",
    "df_silver.printSchema()\n",
    "print(f\"\\nüìä {df_silver.count()} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3dbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde Silver en Parquet\n",
    "df_silver.write.mode(\"overwrite\").parquet(\"s3a://datalake/silver/ventes/\")\n",
    "print(\"‚úÖ Silver sauvegard√© en Parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc8052",
   "metadata": {},
   "source": [
    "## ü•á √âtape 4 : Layer Gold - Agr√©gation (CA par produit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum, count, round as _round\n",
    "\n",
    "# Calcul du CA par produit\n",
    "df_gold = df_silver \\\n",
    "    .groupBy(\"produit\") \\\n",
    "    .agg(\n",
    "        _sum(\"montant_total\").alias(\"chiffre_affaires\"),\n",
    "        _sum(\"quantite\").alias(\"quantite_totale\"),\n",
    "        count(\"id\").alias(\"nombre_ventes\")\n",
    "    ) \\\n",
    "    .withColumn(\"chiffre_affaires\", _round(\"chiffre_affaires\", 2)) \\\n",
    "    .orderBy(col(\"chiffre_affaires\").desc())\n",
    "\n",
    "print(\"ü•á GOLD - CA par produit :\")\n",
    "df_gold.show()\n",
    "print(f\"\\nüìä {df_gold.count()} produits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854036bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde Gold en Parquet\n",
    "df_gold.write.mode(\"overwrite\").parquet(\"s3a://datalake/gold/ca_par_produit/\")\n",
    "print(\"‚úÖ Gold sauvegard√© en Parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc8a9b",
   "metadata": {},
   "source": [
    "## üéâ √âtape 5 : V√©rification finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e306a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"üìÇ STRUCTURE COMPL√àTE DANS MinIO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prefix in ['bronze/', 'silver/', 'gold/']:\n",
    "    print(f\"\\nüìÅ {prefix}\")\n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket='datalake', Prefix=prefix)\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                size_kb = obj['Size'] / 1024\n",
    "                print(f\"   - {obj['Key']} ({size_kb:.2f} KB)\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Vide\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erreur : {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ PIPELINE ETL TERMIN√â AVEC SUCC√àS !\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä R√©sum√© :\")\n",
    "print(f\"   Bronze : {df_bronze.count()} lignes\")\n",
    "print(f\"   Silver : {df_silver.count()} lignes\")\n",
    "print(f\"   Gold   : {df_gold.count()} produits\")\n",
    "\n",
    "# Arr√™t de Spark\n",
    "spark.stop()\n",
    "print(\"\\n‚úÖ SparkSession arr√™t√©e\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
